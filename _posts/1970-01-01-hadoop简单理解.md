> 场景：气象台收集全国各地的气象信息，气象信息原本以格式化的记录形式存储在官方系统中，我们需要处理这些数据

map & reduce
=====
MapReduce任务分为两个阶段，map和reduce，每个阶段都以键值对作为输入和输出。

map阶段：

气象信息的原始数据 --map--> 年份，气温 

reduce阶段：

年份，气温  --reduce--> 最高气温

JavaMapReduce:

```java
public class TestMapper extends Mapper<LongWritable, Text,Text, IntWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String year = line.substring(1,5);
        int temperature = Integer.parseInt(line.substring(6,10));
        //context写入
        context.write(new Text(year),new IntWritable(temperature));
    }
}
```
Mapper<LongWritable, Text, NullWritable, Text>

Mapper范型类，四个参数分别为，入参键，入参值，输出键，输出值。（hadoop本身提供了一套可优化网络序列化传输的基本类型，不使用java基本类型，LongWritable->long;Text->String）
map方法，两个参数为，入参键和入参值，还提供context实例用于输出内容的写入。

```java
public class TestReduce extends Reducer<Text, IntWritable,Text,IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        //map key dosomething,value dosomething 注意这里的参数为map的输出类型
        int finalInt = 0;
        for (IntWritable x: values) {
            finalInt = x.get()+1;
        }
        context.write(key,new IntWritable(finalInt));
    }
}
```
Reduce范型类参数类型与Mapper描述的相同，需要注意的是输入类型需要匹配mapper的输出类型。

运行MapReduce：
```java
public class TestJob {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Job job = Job.getInstance();
        //Set the Jar by finding where a given class came from.
        job.setJarByClass(TestJob.class);
        job.setJobName("job_name");

        //inputs for the map-reduce job.
        FileInputFormat.addInputPath(job,new Path(args[0]));
        //the output directory for the map-reduce job.
        FileOutputFormat.setOutputPath(job,new Path(args[1]));

        //配置mapper和reduce执行类
        job.setMapperClass(TestMapper.class);
        job.setReducerClass(TestReduce.class);

        //设置reduce类型输出
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        System.exit(job.waitForCompletion(true)?0:1);

    }
}
```

mapreduce执行：<br>
```java
//加载hadoop类库
export HADOOP_CLASSPATH=hadoop.jar
//第一个参数为类名，hadoop会开启一个jvm来运行这个类
hadoop TestJob input.txt output
```
job执行结束后，会在output目录下生成reduce任务数量个part-r-xxxx文件
